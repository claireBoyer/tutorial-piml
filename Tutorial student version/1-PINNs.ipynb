{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Physics-informed neural networks"
      ],
      "metadata": {
        "id": "Dvl8v47Qm1N8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1- Introduction to neural networks"
      ],
      "metadata": {
        "id": "dw-i7nSpnEHR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Parametric statistics.** In supervised learning, the goal is to learn a function $f^\\star$ linking the explanatory variable $X$ to the target variable $Y$, i.e., such that $Y = f^\\star(X)+\\epsilon$, where $\\epsilon$ is a random noise.\n",
        "In practice, $f^\\star$ is estimated by $f_{\\hat{\\theta}}$, where $\\hat{\\theta}$ is a minimizer of the empirical risk\n",
        "$$L(\\theta) = \\frac{1}{n}\\sum_{i=1}^n |f_\\theta(X_i)-Y_i|^2 + R(\\theta),$$\n",
        "where\n",
        "* $(X_1, Y_1), \\dots, (X_n,Y_n)$ are i.i.d. observations of $(X,Y)$,\n",
        "* the function class $\\{f_\\theta,\\; \\theta\\in \\Theta\\}$ is chosen to best approximate $f^\\star$,\n",
        "* $R(\\theta)$ is a regularization term (e.g., Ridge, LASSO...) tailored to avoid overfitting."
      ],
      "metadata": {
        "id": "V1jc2kWznpco"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Neural networks.** Neural networks are a kind of parametric functions. They consist of a series of basic operations called *layers*, and the intermediate results of these layers are called the *neurons*. In what follows, we focus on *dense neural networks*, which are a succession of affine transformations and elementwise applications of the hyperbolic tangent (*tanh*) function."
      ],
      "metadata": {
        "id": "kGZouHqF1HGW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1.** Let the function $f: \\mathbb R^2 \\to \\mathbb R$ be defined by\n",
        "$$f(t, x) = B_1 \\tanh(A_1 t + A_2 x + A_3) + B_2 \\tanh(A_4 t + A_5 x + A_6) + B_3 \\tanh(A_7 t + A_8 x + A_9) + B_4.$$\n",
        "\n",
        "a) Show that $f$ is a neural network.\n",
        "\n",
        "b) What is the parameter $\\theta$ such that $f = f_\\theta$?\n",
        "\n",
        "c) How many layers and neurons are there?\n",
        "\n"
      ],
      "metadata": {
        "id": "aqsDIbloiLn9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer.**\n",
        "<details>\n",
        "  <summary>  (Click to show)</summary>\n",
        "a) $f$ can be writen\n",
        "$$f(t,x) = B \\circ \\tanh \\circ A \\begin{pmatrix} t\\\\ x\\end{pmatrix}$$ where\n",
        "\n",
        "*  $A \\begin{pmatrix} t\\\\ x\\end{pmatrix}= \\begin{pmatrix} A_1 & A_2\\\\ A_4 & A_5 \\\\ A_7 & A_8\\end{pmatrix} \\begin{pmatrix} t\\\\ x\\end{pmatrix} + \\begin{pmatrix} A_3\\\\ A_6\\\\ A_9\\end{pmatrix},$ is an affine transformation\n",
        "* $\\tanh (x_1, x_2, x_3) = (\\tanh(x_1), \\tanh(x_2), \\tanh(x_3))$ is the elementwise application of the hyperbolic tangent,\n",
        "* the neurons $z_1, z_2, z_3$ of the first layer are defined by $\\begin{pmatrix} z_1\\\\ z_2\\\\z_3\\end{pmatrix} =  \\tanh \\circ A \\begin{pmatrix} t\\\\ x\\end{pmatrix}$,\n",
        "* and\n",
        "$B \\begin{pmatrix} z_1\\\\ z_2\\\\z_3\\end{pmatrix}= \\begin{pmatrix} B_1 & B_2 & B_3\\end{pmatrix} \\begin{pmatrix} z_1\\\\ z_2\\\\ z_3\\end{pmatrix} + B_4$ is an affine transformation.\n",
        "\n",
        "b) Here, the parameter $\\theta$ are the parameters $(A_1, \\dots, A_9, B_1, \\dots, B_4)$ of the affine transformations.\n",
        "\n",
        "c) Since the output space of $A: \\mathbb R^2 \\to \\mathbb R^3$ is of dimension 3, there are 3 neurons in the first layers. Since we apply the activation function, tanh, only once, there is only one layer. Therefore, there is one layer with three neurons.\n",
        "</details>"
      ],
      "metadata": {
        "id": "e439LwkNiZup"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neural networks are popular because they are a class of nonlinear functions, and computing their derivatives can be done efficiently using the so-called *backpropagation* algorithm. Most neural networks are implemented using the *Python* programming language. Currently, the two most popular deep learning libraries are *PyTorch* (torch) and *Tensorflow* (tf). In this tutorial, we will use PyTorch."
      ],
      "metadata": {
        "id": "81mYL6gTiNMl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "-bkYkJ3AERag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In what follows, we will rely heavily on the generation of random numbers. For example, when initializing a neural network, the weights are sampled according to a Gaussian distribution.\n",
        "To ensure reproducibility of the results, we must keep track of the random numbers generated. We define a number, called a *seed*, which completely determines how the random number generator will be initialized.\n",
        "Using the same seed will produce the same outputs."
      ],
      "metadata": {
        "id": "EXjHLL5ZETD5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(314)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBFj4BSoEYmW",
        "outputId": "1a9463e4-8f3e-4b2c-9bd3-852c7e2ac1f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7b06b8547090>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdMeoptxmvm-"
      },
      "outputs": [],
      "source": [
        "# The next line means that we define a new type, called \"NeuralNet\", based on the\n",
        "# structure of the neural network \"nn.Module\".\n",
        "class NeuralNet(nn.Module):\n",
        "\n",
        "# This new object will come with two functions \"__init__\" and \"forward\".\n",
        "# \"__init__\" is useful to give an initial value to the variable \"NeuralNet\".\n",
        "\n",
        "    def __init__(self):\n",
        "        super(NeuralNet, self).__init__()\n",
        "\n",
        "        # Here, we define the structure of the neural network.\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(2, 10),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(10, 10),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(10, 10),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(10, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, t, x):\n",
        "        input = torch.cat([t, x], dim=1)\n",
        "        return self.net(input)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2.** Detail the architecture of this neural network."
      ],
      "metadata": {
        "id": "el9AXNUHaYVv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer.**\n",
        "<details>\n",
        "  <summary>  (Click to show)</summary>\n",
        "This neural network is a *dense* neural network with 3 layers, since we apply the activation function $\\tanh$ three times. Each layer contains 10 neurons.\n",
        "</details>"
      ],
      "metadata": {
        "id": "U8mqoRBcaicO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GPU programming.** Graphics processing units (GPUs) are specialized electronic circuits designed to run some basic linear algebra operations more efficiently than cheaper, more common central processing units (CPUs). GPUs rely on both parallelization and efficient memory allocation. Pytorch provides a very simple way to run code on a GPU. The following command automatically detects whether a GPU is available, and sets the *device* variable to either the available GPU, or the CPU if no GPU is available."
      ],
      "metadata": {
        "id": "togSPg-81LG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Detect a NVIDIA GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "O14dUurxnLL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For MacOS users who want to use their M1, M2, or M3 GPU, the command is as follows."
      ],
      "metadata": {
        "id": "Oy6JaYmnCf7h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Detect a GPU on MacOS computers\n",
        "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "lX5Y1JSSCrjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, the following line automatically samples a 2x2 array of uniform random variables on the *device*."
      ],
      "metadata": {
        "id": "p_1fJptwA-0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_variable = torch.rand(2, 2, device=device)\n",
        "print(test_variable)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRg_CUpfA6F-",
        "outputId": "a5fc9136-f83b-4394-8897-b7c99d24e774"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.7196, 0.6295],\n",
            "        [0.6667, 0.3385]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note how lists of numbers are referred to as *tensors* in the PyTorch framework.\n",
        "Tensors can represent many objects of linear algebra. For example,\n",
        "\n",
        "* the vector $(1, 4)$ corresponds to $\\hbox{tensor}([1, 4])$ of shape $(2,)$,\n",
        "* the vector $\\begin{pmatrix}1\\\\ 4\\end{pmatrix}$  corresponds to $\\hbox{tensor}([[1], [4]])$ of shape $(2,1)$,\n",
        "* the matrix $\\begin{pmatrix}1& 3\\\\ 4& -1\\end{pmatrix}$ corresponds to $\\hbox{tensor}([[1, 3], [4, -1]])$ of shape $(2,2)$.\n",
        "\n",
        "We say that a tensor is of size $(*, 1)$ if its size is of the form $(n, 1)$.\n",
        "Tensors supports various types of linear functions like\n",
        "\n",
        "* addition and subtraction, encodes by the symbols + and -\n",
        "* matrix-matrix and matrix-vector multiplications, encoded by the symbol @\n",
        "* elementwise multiplication, encoded by the symbol *"
      ],
      "metadata": {
        "id": "1exyj5vUUTs2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2- PDE solving with PINNs"
      ],
      "metadata": {
        "id": "h1AdKxVDnLmW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PDE solving.** Here, we apply the tools from parametric statistics with neural networks to solve PDEs. To give a concrete example, let's solve the heat equation\n",
        "$$\\partial_1 f = \\partial^2_{2,2} f$$\n",
        "on the square $[0,1]^2$, with\n",
        "* the boundary conditions $$\\forall t\\in [0,1],\\; f(t, 0) = f(t, 1) = 0,$$\n",
        "* and the initial conditions $$\\forall x\\in [0,1],\\; f(0, x) = \\sin(\\pi x).$$"
      ],
      "metadata": {
        "id": "71R_QOfCxUmx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To solve this PDE, the *physics-informed neural networks* (PINNs) approach consists in minimizing the loss\n",
        "$$L(\\theta) = \\frac{1}{n_i}\\sum_{i=1}^{n_i}(f_\\theta(0, X^{(ini)}_i)-\\sin(\\pi X^{(ini)}_i))^2 + \\frac{1}{n_b}\\sum_{j=1}^{n_b}f_\\theta(T^{(b)}_j, 0)^2 + \\frac{1}{n_c}\\sum_{k=1}^{n_c}(\\partial_t f_\\theta(T^{(c)}_k, X^{(c)}_k)-\\partial^2_{x,x} f_\\theta(T^{(c)}_k, X^{(c)}_k))^2,$$\n",
        "where the points $X^{(ini)}_i$, $T^{(b)}_j$, $T^{(c)}_k$, and $X^{(c)}_k$ are independent and sampled along the uniform distribution on $[0,1]$."
      ],
      "metadata": {
        "id": "dhTijVfxH0CT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3.** Given a fixed parameter $\\theta$, what is the limit of $L(\\theta)$ as $n_i, n_b, n_c \\to \\infty$?"
      ],
      "metadata": {
        "id": "_FNL3uNjaPFi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer.**\n",
        "<details>\n",
        "  <summary>  (Click to show)</summary>\n",
        "The law of large number states that the empirical risk $L(\\theta)$ converges almost surely to the theoretical risk $R(\\theta)$ defined by\n",
        "$$\\lim_{n_b, n_c \\to \\infty} L(\\theta) = R(\\theta) = \\int_0^1(f_\\theta(0, x)-\\sin(\\pi x))^2dx + \\int_0^1 f_\\theta(t, 0)^2dt + \\int_0^1(\\partial_t f_\\theta(t,x)-\\partial^2_{x,x} f_\\theta(t,x))^2dtdx.$$\n",
        "</details>"
      ],
      "metadata": {
        "id": "3jeNPL1nbIl8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4.** Let $R(\\theta) = \\lim_{n_i, n_b, n_c\\to \\infty} L(\\theta)$. Why does minimizing $R$ amounts to solving the PDE?"
      ],
      "metadata": {
        "id": "Ui32Z5Nnb8IO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer.**\n",
        "<details>\n",
        "  <summary>  (Click to show)</summary>\n",
        "Note that $\\forall \\theta, \\;R(\\theta) \\geq 0$. Morevoer, $R(\\theta) = 0$ if and only if $f_\\theta$ satifies the initial conditions, the boundary conditions, and the PDE.\n",
        "</details>"
      ],
      "metadata": {
        "id": "xVXj4XPVcd4f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5.** Implement a function *pde_residual* computing the PDE residual $\\partial_t u - \\partial^2_{x,x}u$, where $u$ is a neural network. To do so, use the following framework of Pytorch, which allows to compute the derivarive of $u$ with respect to a variable $t$."
      ],
      "metadata": {
        "id": "uO0Tq_eUdCp6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PDE residual\n",
        "def pde_residual(model, t, x):\n",
        "    # t and x are tensors of size (*, 1)\n",
        "    u = model(t, x) # Evaluate the neural network u at the values where you want to compute the derivatives\n",
        "    t.requires_grad_(True) # Allows to compute the derivative with respect to the variable t\n",
        "\n",
        "    u_t = torch.autograd.grad(u, t, grad_outputs=torch.ones_like(u), retain_graph=True, create_graph=True)[0]\n",
        "    # Compute the gradient of u with respect to t.\n",
        "    # Grad_outputs specify the shape of the gradient.\n",
        "    # retain_graph allows to differentiate multiple times.\n",
        "\n",
        "    u_x = ...\n",
        "    u_xx = ...\n",
        "\n",
        "    return u_t - u_xx"
      ],
      "metadata": {
        "id": "THHwa6D0eGI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 6.** Implement a function that generates the training data using the following functions from the *torch* library: *rand*, *randn*, *zeros*, *ones*, *linspace*, and *sin*. As is typical with neural networks, all tensors must be of shape $(*, 1)$."
      ],
      "metadata": {
        "id": "9dXHDLoifZ0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training data\n",
        "def generate_training_data(N_f=10**4, N_b=10**2, N_i=10**2):\n",
        "    # Collocation points (interior)\n",
        "    t_f = torch.rand(N_f, 1, device=device)\n",
        "    x_f = ...\n",
        "\n",
        "    # Boundary conditions\n",
        "    t_b = ...\n",
        "    x_b0 = ...\n",
        "    x_b1 = ...\n",
        "\n",
        "    # Initial condition\n",
        "    x_i = ...\n",
        "    t_i = ...\n",
        "    u_i = ...\n",
        "\n",
        "    return t_f, x_f, t_b, x_b0, x_b1, t_i, x_i, u_i"
      ],
      "metadata": {
        "id": "TIfYiC7Sf_1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 7.** Implement the gradient descent to train the neural network."
      ],
      "metadata": {
        "id": "gie2d7I3gpeM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "def train(model, epochs=5000, lr=1e-3):\n",
        "    model.train()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    t_f, x_f, t_b, x_b0, x_b1, t_i, x_i, u_i = generate_training_data(N_f=10**4, N_b=10**2, N_i=10**2)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # PDE residual loss\n",
        "        loss_f = ...\n",
        "\n",
        "        # Boundary condition loss\n",
        "        loss_b = ...\n",
        "\n",
        "        # Initial condition loss\n",
        "        loss_i = ...\n",
        "\n",
        "        loss = loss_f + loss_b + loss_i\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch % 500 == 0:\n",
        "            print(f\"Epoch {epoch}, Loss: {loss.item():.4e}\")\n",
        "\n",
        "    print(\"Training complete.\")"
      ],
      "metadata": {
        "id": "RV3oIxPugzOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8.** Reset the seed to 314. Then, train your neural network."
      ],
      "metadata": {
        "id": "czoKuhV6hHzU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WSQnYBgLbWoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Analytical solutions.**\n",
        "To estimate the performance of the PINN, we can compare its outputs with the unique solution $f^\\star$ of the PDE $$f^\\star(t,x) = \\exp(-\\pi^2 t)\\sin(\\pi x).$$"
      ],
      "metadata": {
        "id": "GGqxv33EyhhR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 9.** Check that $f^\\star$ is indeed a solution of the PDE."
      ],
      "metadata": {
        "id": "YtjCuiUahrtM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer.**\n",
        "<details>\n",
        "  <summary>  (Click to show)</summary>\n",
        "On the one hand,\n",
        "$$\\partial_t f = -\\pi^2 f.$$\n",
        "On the other hanf,\n",
        "$$\\partial^2_{x,x} f = -\\pi^2 f.$$\n",
        "Thus, $\\partial_t f = \\partial^2_{x,x} f$.\n",
        "</details>"
      ],
      "metadata": {
        "id": "2j9Aofbfhz8R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10.** Using three plots, plot the functions $f^\\star$, the PINN, and the error $|\\hbox{PINN}-f^\\star|$. Compute the Mean Average Error (MAE) $$MAE = \\frac{1}{100^2}\\sum_{k=0}^{100} \\sum_{\\ell=0}^{100} |\\hbox{PINN}(k/100, \\ell/100)-f^\\star(k/100, \\ell/100)|.$$"
      ],
      "metadata": {
        "id": "4i_ijxeHicwF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot(model):\n",
        "  ..."
      ],
      "metadata": {
        "id": "4SOPmMJYzwNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot(model)"
      ],
      "metadata": {
        "id": "I2fE_Slp8tgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3- Some basic improvements over vanilla PINNs\n",
        "\n",
        "There are many ways to improve this implementation, but which complicates the understanding of the code baseline.\n",
        "For instance, instead of fixing the collocation points, and the points penalizing the boundary and initial conditions, we can sample new points at each epoch. This has the advantage of both increasing the number of available data while reducing overfitting."
      ],
      "metadata": {
        "id": "48zd0eQ-7I4E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 11.** Implement this PINN where data are resampled at each epoch."
      ],
      "metadata": {
        "id": "7lBqCrVVjcbp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_2(model, epochs=5000, lr=1e-3):\n",
        "\n"
      ],
      "metadata": {
        "id": "bxuKt58_8KJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 12.** Train your model and compare its MAE with the other PINN."
      ],
      "metadata": {
        "id": "iO67OzQnjt2x"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3tJry4Uh8r4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4- Hybrid modelling"
      ],
      "metadata": {
        "id": "8q9pTTdanQLQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hybrid modelling.** Here, we assume not to know the boundary conditions entirely. Instead, we have at hand $n$ i.i.d. observations $(T_1, X_1, Y_1)$, $\\dots$, $(T_n, X_n, Y_n)$ such that\n",
        "\n",
        "* $(T, X$) follows the uniform distribution on $[0,1]^2$\n",
        "* $Y = f^\\star(T, X)+\\epsilon$,  \n",
        "* and $f^\\star$ is a solution to the heat equation $$\\partial_1 f = \\partial^2_{2,2} f.$$\n",
        "\n",
        "In the next example, the unknown function $f^\\star$ is once again\n",
        "$$f^\\star(t,x) = \\exp(-\\pi^2t)\\sin(\\pi x)$$\n",
        "and $\\epsilon$ is a gaussian noise $N(0,1)$."
      ],
      "metadata": {
        "id": "_bqNWsv9JRQN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 13.** Which empirical loss corresponds to this problem?"
      ],
      "metadata": {
        "id": "jLkIM7Y882_T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer.**\n",
        "<details>\n",
        "  <summary>  (Click to show)</summary>\n",
        "$$L(\\theta) = \\frac{1}{n}\\sum_{i=1}^{n}(f_\\theta(T_i, X_i)-Y_i))^2 + \\frac{\\lambda_n}{n_c}\\sum_{k=1}^{n_c}(\\partial_t f_\\theta(T^{(c)}_k, X^{(c)}_k)-\\partial^2_{x,x} f_\\theta(T^{(c)}_k, X^{(c)}_k))^2,$$\n",
        "where the points $T^{(c)}_k$ and $X^{(c)}_k$ are independent and sampled along the uniform distribution on $[0,1]$. $\\lambda_n \\geq 0$ is an hyperparameter weighting the importance of the PDE penalty.\n",
        "</details>"
      ],
      "metadata": {
        "id": "7OBvufZT872b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 14.** Implement a function to generate the training data using the functions *rand*, *randn*, *zeros*, *ones*, *linspace* and *sin* of the *torch* library. As usual with neural networks, all tensors must be of shape $(*, 1)$."
      ],
      "metadata": {
        "id": "vtim5VLD7iFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training data\n",
        "def generate_training_data_2(N_f=10**4, N_data=10**2, sigma=0.1):\n",
        "    # Collocation points (interior)\n",
        "    t_f = ...\n",
        "    x_f = ...\n",
        "\n",
        "    # Data points\n",
        "    t = ...\n",
        "    x = ...\n",
        "    y = ...\n",
        "\n",
        "    return t_f, x_f, t, x, y\n"
      ],
      "metadata": {
        "id": "v13rZW6a79qS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 15.** To assess the impact of using the physical a priori on the learning performance, implement two algorithms: one which uses the physical penalty (i.e., $\\lambda_n > 0$) and one which does not (i.e., $\\lambda_n = 0$)."
      ],
      "metadata": {
        "id": "uVKFqDxk8dIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_without_PDE(model, epochs=5000, lr=1e-3):\n",
        "\n",
        "def train_hybrid(model, epochs=5000, lr=1e-3):\n",
        ""
      ],
      "metadata": {
        "id": "iVHYboRwnNdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 16.** Compare their performance."
      ],
      "metadata": {
        "id": "jQdzi-ni9qE1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fEhi_aPISBxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5- Going Further\n",
        "\n",
        "PINNs are interesting because they adapt seamlessly to many contexts in PDE solving and hybrid modelling without  increasing the complexity of the algorithm.\n",
        "\n",
        "\n",
        "**Question 17: Nonlinear PDEs** Nonlinear PDEs are notoriously difficult to solve. However, implementing PINNs to solve nonlinear PDEs is straightforward.  Implement a PINN to solve the Burgers equation $$\\partial_1 f + f \\partial_2 f = 0$$ on the square $[0,1]$ with initial conditions $f(0,x) = 1+x$ and boundary conditions $$f(t,0) = \\frac{1}{1+t}.$$ Compare it with the values of the unique solution $f(t,x) = \\frac{1+x}{1+t}$.\n",
        "(Hint: You only need to change the function *pde_residual* and to get rid of the boundary and initial conditions)."
      ],
      "metadata": {
        "id": "NKPOWIoNJzrP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 18: General geometries.** Solving PDEs on complex geometries often results in complicated meshes when using classical techniques However, PINNs can be easily implemented to solve PDEs on general geometries. Implement a PINN to solve the Laplace equation $$\\partial^2_{1,1} f + \\partial^2_{2,2} f + \\partial^3_{3,3} f = 6$$ on the ball $\\{(x_1, x_2, x_3)\\in \\mathbb R^3, \\; x_1^2 + x_2^2+x_3^3 \\leq 1\\}$ with boundary conditions\n",
        "* $f(x_1,x_2,x_3) = x_1^2+x_2^2+x_3^2$ if $x_1^2+x_2^2+x_3^2 = 1$.\n",
        "\n",
        "Compare these values with those of the solution $f(x_1,x_2, x_3) = x_1^2+x_2^2+x_3^2$."
      ],
      "metadata": {
        "id": "igKBQvmoj69z"
      }
    }
  ]
}